{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas è una libreria software scritta per il linguaggio di programmazione Python per la manipolazione e l'analisi dei dati. In particolare, offre strutture dati e operazioni per manipolare tabelle numeriche e serie temporali.\n",
    "import pandas as pd\n",
    "#NumPy è una libreria open source per il linguaggio di programmazione Python, che aggiunge supporto a grandi matrici e array multidimensionali insieme a una vasta collezione di funzioni matematiche di alto livello per poter operare efficientemente su queste strutture dati.\n",
    "import numpy as np\n",
    "#Matplotlib è una libreria per la creazione di grafici per il linguaggio di programmazione Python e la libreria matematica NumPy. Fornisce API orientate agli oggetti che permettono di inserire grafici all'interno di applicativi usando toolkit GUI generici, come WxPython, Qt o GTK.\n",
    "import matplotlib.pyplot as plt\n",
    "#Importo il modello di regressione lineare\n",
    "#Il nostro obiettivo è confrontare la funzione scritta da noi con una implementata da una libreria (sklearn). Quindi prima la implementiamo manualmente e poi la confrontiamo con quella di sklearn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Modulo contenente Axes3D, un oggetto che può tracciare oggetti 3D su una figura matplotlib 2D.\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduzione\n",
    "In questa parte, implementeremo la regressione lineare con più variabili per prevedere i prezzi delle case. Supponiamo che stai vendendo la tua casa e vuoi sapere quale sarebbe un buon prezzo di mercato. Un modo per farlo è quello di raccogliere prima informazioni sulle case recenti vendute e creare un modello dei prezzi delle case. Il set di dati contiene un set di formazione sui prezzi delle abitazioni a Portland, Oregon. La prima colonna è la dimensione della casa (in piedi quadrati), la seconda colonna è il numero di camere da letto e la terza colonna è il prezzo della casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.104e+03 3.000e+00 3.999e+05]\n",
      " [1.600e+03 3.000e+00 3.299e+05]\n",
      " [2.400e+03 3.000e+00 3.690e+05]\n",
      " [1.416e+03 2.000e+00 2.320e+05]\n",
      " [3.000e+03 4.000e+00 5.399e+05]]\n",
      "(47, 3)\n"
     ]
    }
   ],
   "source": [
    "#Carico i dati contenuti nel file che saranno splittati dal delimitatore ,\n",
    "#Il dataset contiene i dati della popolazione in migliaia ed il profitto\n",
    "data = np.loadtxt('data/ex1data2.txt', delimiter=',')\n",
    "\n",
    "#Stampo i primi 10 elementi e tutte le colonne\n",
    "print(data[:5,:])\n",
    "\n",
    "#Stampo lo shape dei dati, cioè le dimensioni della struttura dati\n",
    "print(data.shape)\n",
    "#Prima colonna dimensioni casa in feat\n",
    "#Seconda colonna numero di stanze da letto per ogni casa\n",
    "#Terza colonna prezzo casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprendimento Supervisionato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :2]\n",
    "y = data[:, -1]\n",
    "\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "#Passo dall' n-dimensionale alla matrice m x 1\n",
    "#np.reshape(y, (y.shape[0], 1)) equivale a np.reshape(-1,1)\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "#print(\"Training examples : {}\".format(m))\n",
    "#print(\"Features : {}\".format(n))\n",
    "\n",
    "#print(\"First 10 examples: \\n\", X[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th example; Input features [2104.    3.] Target value [399900.]\n",
      "2-th example; Input features [1600.    3.] Target value [329900.]\n",
      "3-th example; Input features [2400.    3.] Target value [369000.]\n",
      "4-th example; Input features [1416.    2.] Target value [232000.]\n",
      "5-th example; Input features [3000.    4.] Target value [539900.]\n",
      "6-th example; Input features [1985.    4.] Target value [299900.]\n",
      "7-th example; Input features [1534.    3.] Target value [314900.]\n",
      "8-th example; Input features [1427.    3.] Target value [198999.]\n",
      "9-th example; Input features [1380.    3.] Target value [212000.]\n",
      "10-th example; Input features [1494.    3.] Target value [242500.]\n",
      "11-th example; Input features [1940.    4.] Target value [239999.]\n",
      "12-th example; Input features [2000.    3.] Target value [347000.]\n",
      "13-th example; Input features [1890.    3.] Target value [329999.]\n",
      "14-th example; Input features [4478.    5.] Target value [699900.]\n",
      "15-th example; Input features [1268.    3.] Target value [259900.]\n",
      "16-th example; Input features [2300.    4.] Target value [449900.]\n",
      "17-th example; Input features [1320.    2.] Target value [299900.]\n",
      "18-th example; Input features [1236.    3.] Target value [199900.]\n",
      "19-th example; Input features [2609.    4.] Target value [499998.]\n",
      "20-th example; Input features [3031.    4.] Target value [599000.]\n",
      "21-th example; Input features [1767.    3.] Target value [252900.]\n",
      "22-th example; Input features [1888.    2.] Target value [255000.]\n",
      "23-th example; Input features [1604.    3.] Target value [242900.]\n",
      "24-th example; Input features [1962.    4.] Target value [259900.]\n",
      "25-th example; Input features [3.89e+03 3.00e+00] Target value [573900.]\n",
      "26-th example; Input features [1100.    3.] Target value [249900.]\n",
      "27-th example; Input features [1458.    3.] Target value [464500.]\n",
      "28-th example; Input features [2526.    3.] Target value [469000.]\n",
      "29-th example; Input features [2200.    3.] Target value [475000.]\n",
      "30-th example; Input features [2637.    3.] Target value [299900.]\n",
      "31-th example; Input features [1839.    2.] Target value [349900.]\n",
      "32-th example; Input features [1000.    1.] Target value [169900.]\n",
      "33-th example; Input features [2040.    4.] Target value [314900.]\n",
      "34-th example; Input features [3.137e+03 3.000e+00] Target value [579900.]\n",
      "35-th example; Input features [1811.    4.] Target value [285900.]\n",
      "36-th example; Input features [1437.    3.] Target value [249900.]\n",
      "37-th example; Input features [1239.    3.] Target value [229900.]\n",
      "38-th example; Input features [2132.    4.] Target value [345000.]\n",
      "39-th example; Input features [4.215e+03 4.000e+00] Target value [549000.]\n",
      "40-th example; Input features [2162.    4.] Target value [287000.]\n",
      "41-th example; Input features [1664.    2.] Target value [368500.]\n",
      "42-th example; Input features [2238.    3.] Target value [329900.]\n",
      "43-th example; Input features [2567.    4.] Target value [314000.]\n",
      "44-th example; Input features [1200.    3.] Target value [299000.]\n",
      "45-th example; Input features [852.   2.] Target value [179900.]\n",
      "46-th example; Input features [1852.    4.] Target value [299900.]\n",
      "47-th example; Input features [1203.    3.] Target value [239500.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(m):\n",
    "    print(\"{}-th example; Input features {} Target value {}\".format((i+1), X[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling delle features (normalizzazione in forma di scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poichè le features differiscono in ordine di grandezza bisogna effettuare uno scaling che permette una più veloce convergenza\n",
    "con il gradient descent. La funzione featureNormalize() esegue le seguenti procedure:\n",
    "    1) Sottrae il valore medio di ogni feature dal dataset;\n",
    "    2) Dopo aver sottratto la media, ridimensionare ulteriormente (scale o divide) le features values per la rispettiva deviazione standard. \n",
    "    La deviazione standard è un modo per misurare quanta variazione ci sia nell'intervallo di valori di una particolare \n",
    "    caratteristica (la maggior parte dei punti di dati si troveranno entro +-2 della deviazione standard della media); questa \n",
    "    è un'alternativa alla presa dell'intervallo di valori (max-min). Nel momento in cui viene chiamato featureNormalize(), \n",
    "    la colonna aggiuntiva di 1 corrispondente a x0 = 1 non è stata ancora aggiunta a X.\n",
    "    \n",
    "### Note implementative\n",
    "Durante la normalizzazione delle funzioni, è importante memorizzare i valori utilizzati per la normalizzazione: il valore medio e la deviazione standard utilizzata per i calcoli. Dopo aver appreso i parametri dal modello, spesso vogliamo prevedere i prezzi delle case che non abbiamo mai visto prima. Dato un nuovo valore x (area soggiorno e numero di camere da letto), dobbiamo prima normalizzare x usando la deviazione media e standard precedentemente calcolata dal set di addestramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large i = 0,...,m $$\n",
    "$$ \\large j = 0,...,n $$\n",
    "$$ \\large x_{j}^{(i)} = \\frac{x_{j}^{(i)}-\\mu_{j}}{\\sigma_{j}} $$\n",
    "$$ \\large x_{j}^{(i)} = \\frac{x_{j}^{(i)}-\\mu_{j}}{max_{j} - min_{j}} $$\n",
    "$$ \\sigma->(deviazione-standard) $$ \n",
    "$$ \\mu->(media) $$ \n",
    "$$ \\max->(max) $$\n",
    "$$ \\min->(min) $$\n",
    "$$ j->colonna $$\n",
    "$$ i->i-esimo-training-example $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accetto in input X\n",
    "def featureScaling(X):\n",
    "    X_scaled = X\n",
    "    \n",
    "    #Inizializzo queste 4 variabili ai valori di media, ds, max e min; le quali hanno 1 riga ed il numero di colonne pari ad X (2 nel nostro caso)\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    maxima = np.amax(X, axis=0)\n",
    "    minima = np.amin(X, axis=0)\n",
    "\n",
    "    for j in range(X.shape[1]):\n",
    "        X_scaled[:,j] = (X[:,j] - mu[j])/sigma[j]\n",
    "        #X_scaled[:,j] = (X[:,j] - mu[j])/(maxima[j] - minima[j])\n",
    "        \n",
    "    return X_scaled, mu, sigma, maxima, minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N.B. Bisogna prima effettuare lo scaling e poi aggiungere la colonna di 1 per pesare le features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridimensione l'input X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.104e+03 3.000e+00]\n",
      " [1.600e+03 3.000e+00]\n",
      " [2.400e+03 3.000e+00]\n",
      " [1.416e+03 2.000e+00]\n",
      " [3.000e+03 4.000e+00]\n",
      " [1.985e+03 4.000e+00]\n",
      " [1.534e+03 3.000e+00]\n",
      " [1.427e+03 3.000e+00]\n",
      " [1.380e+03 3.000e+00]\n",
      " [1.494e+03 3.000e+00]\n",
      " [1.940e+03 4.000e+00]\n",
      " [2.000e+03 3.000e+00]\n",
      " [1.890e+03 3.000e+00]\n",
      " [4.478e+03 5.000e+00]\n",
      " [1.268e+03 3.000e+00]\n",
      " [2.300e+03 4.000e+00]\n",
      " [1.320e+03 2.000e+00]\n",
      " [1.236e+03 3.000e+00]\n",
      " [2.609e+03 4.000e+00]\n",
      " [3.031e+03 4.000e+00]\n",
      " [1.767e+03 3.000e+00]\n",
      " [1.888e+03 2.000e+00]\n",
      " [1.604e+03 3.000e+00]\n",
      " [1.962e+03 4.000e+00]\n",
      " [3.890e+03 3.000e+00]\n",
      " [1.100e+03 3.000e+00]\n",
      " [1.458e+03 3.000e+00]\n",
      " [2.526e+03 3.000e+00]\n",
      " [2.200e+03 3.000e+00]\n",
      " [2.637e+03 3.000e+00]\n",
      " [1.839e+03 2.000e+00]\n",
      " [1.000e+03 1.000e+00]\n",
      " [2.040e+03 4.000e+00]\n",
      " [3.137e+03 3.000e+00]\n",
      " [1.811e+03 4.000e+00]\n",
      " [1.437e+03 3.000e+00]\n",
      " [1.239e+03 3.000e+00]\n",
      " [2.132e+03 4.000e+00]\n",
      " [4.215e+03 4.000e+00]\n",
      " [2.162e+03 4.000e+00]\n",
      " [1.664e+03 2.000e+00]\n",
      " [2.238e+03 3.000e+00]\n",
      " [2.567e+03 4.000e+00]\n",
      " [1.200e+03 3.000e+00]\n",
      " [8.520e+02 2.000e+00]\n",
      " [1.852e+03 4.000e+00]\n",
      " [1.203e+03 3.000e+00]]\n",
      "Mean : [2000.68085106    3.17021277]\n",
      "Standard Deviation : [7.86202619e+02 7.52842809e-01]\n",
      "Max : [4478.    5.]\n",
      "Min : [852.   1.]\n"
     ]
    }
   ],
   "source": [
    "X_scaled, mu_X, sigma_X, maxima_X, minima_X = featureScaling(np.copy(X))\n",
    "\n",
    "print(\"Mean : {}\".format(mu_X))\n",
    "print(\"Standard Deviation : {}\".format(sigma_X))\n",
    "print(\"Max : {}\".format(maxima_X))\n",
    "print(\"Min : {}\".format(minima_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono due metodologie per trovare il modello migliore valuntando il costo ed aggiornando i pesi:\n",
    "    1)Con il Gradient Descent per il quale abbiamo bisogno di scegliere l'iperparametro alfa e settare un grande numero di iterazioni.\n",
    "    Il GD è da scegliere quando è presente un grande numero di features, 10^6 è un numero ragionevole di features.\n",
    "    2)Con la Normal Equation per la quale non c'è bisogno di scegliere alfa e non sono presenti iterazioni visto il calcolo one shot sui\n",
    "    pesi per poi valutare la Cost function. Per un numero di features elevato il costo computazionale risulta essere troppo elevato, \n",
    "    meglio se usata con piccoli dataset 100 o 1000 features.\n",
    "N.B. Nella Normal Equation c'è bisogno che X^T * X sia invertibile, essa non lo è quando abbiamo features ridondanti o quando si hanno \n",
    "molte features rispetto ai training examples (la Regularization ovvia tale problema)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridimensiono l'output y (Solo per scopo visualizzativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : [340412.65957447]\n",
      "Standard Deviation : [123702.53600615]\n",
      "Max : [699900.]\n",
      "Min : [169900.]\n"
     ]
    }
   ],
   "source": [
    "y_scaled, mu_y, sigma_y, maxima_y, minima_y = featureScaling(np.copy(y))\n",
    "\n",
    "print(\"Mean : {}\".format(mu_y))\n",
    "print(\"Standard Deviation : {}\".format(sigma_y))\n",
    "print(\"Max : {}\".format(maxima_y))\n",
    "print(\"Min : {}\".format(minima_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo la funzione di costo\n",
    "#X input\n",
    "#y = output\n",
    "#Se non passo esplicitamente theta, theta è settata di default come un vettore nullo (di 0) di 2 righe ed una colonna\n",
    "def computeCostVectorized(X, y, theta=np.zeros((X.shape[1],1))):\n",
    "    #Calcolo il numero di examples\n",
    "    m = y.size\n",
    "    \n",
    "    #Setto una variabile per la funzione di costo\n",
    "    J = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    h = X.dot(theta)\n",
    "    J = 1/(2*m)*((h-y).T.dot(h-y))\n",
    "    end = time.time()\n",
    "    eta = end -start\n",
    "    \n",
    "    return(J, eta)\n",
    "\n",
    "#Il tempo rappresentato sotto è il tempo trascorso per eseguire la cella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function (Loop-based Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostLoopNumpy(X, y, theta=np.zeros((X.shape[1], 1))):\n",
    "    m = y.size\n",
    "    J = 0\n",
    "    h = np.zeros((m,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # hypothesis evaluation loop-based implementation\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            #h[i] = h[i] + theta[j] * X[i,j]\n",
    "            h[i] += theta[j] * X[i,j]\n",
    "    \n",
    "    # semi-vectorized implementation (Numpy)\n",
    "    J = 1/(2*m)*np.sum(np.square(h-y))\n",
    "    end = time.time()\n",
    "    eta = end-start\n",
    "    return(J, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.55915481e+10]]), 0.0005841255187988281)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passo X.shape[1] in modo da non vincolare la grandezza sul numero di colonne\n",
    "computeCostVectorized(X,y,theta=np.zeros((X.shape[1],1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Hypothesis for Contour plots \n",
    "$$ \\theta_{0}=0 $$\n",
    "$$ h_{0}(x)=\\theta_{1}x_{1} + \\theta_{1}x_{2} $$\n",
    "Sacrifichiamo theta0 per avere un plot capibile, altrimenti i plot sarebbero\n",
    "3D e difficili da comprendere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Plots (con theta1 e theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = np.linspace(-500, 700, 100)\n",
    "B2 = np.linspace(-500000, 500000, 100)\n",
    "\n",
    "BS1 = np.linspace(-10, 10, 100)\n",
    "BS2 = np.linspace(-10, 10, 100)\n",
    "\n",
    "#Creo la griglia bidimensionale indicizzata xy\n",
    "xx, yy = np.meshgrid(B1, B2, indexing='xy')\n",
    "xxs, yys = np.meshgrid(BS1, BS2, indexing='xy')\n",
    "Z = np.zeros((B1.size, B2.size))\n",
    "Z_scaled = np.zeros((BS1.size, BS2.size))\n",
    "\n",
    "for(i,j),v in np.ndenumerate(Z):\n",
    "    Z[i,j], _ = computeCostVectorized(X,y,[[xx[i,j]],[yy[i,j]]])\n",
    "    \n",
    "for(i,j),v in np.ndenumerate(Z):\n",
    "    Z_scaled[i,j], _ = computeCostVectorized(X_scaled,y_scaled,[[xxs[i,j]],[yys[i,j]]])\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(30,10))\n",
    "ax1=fig.add_subplot(121)\n",
    "ax2=fig.add_subplot(122)\n",
    "\n",
    "#Logspace Restituisce uniformemente gli spazi numerici con intervallo w.r.t su una log scale.\n",
    "CS_1 = ax1.contour(xx, yy, Z, np.logspace(-300,300,3000), cmap=plt.cm.jet, alpha=0.6)\n",
    "ax1.set_title('Contour Plot (Without Feature Scaling)')\n",
    "\n",
    "CS_2 = ax2.contour(xx, yy, Z_scaled, np.logspace(-100,300,3000), cmap=plt.cm.jet, alpha=0.6)\n",
    "ax2.set_title('Contour Plot (With Feature Scaling)')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r'$\\theta_1$', fontsize=17)\n",
    "    ax.set_ylabel(r'$\\theta_2$', fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Vectorized\n",
    "y non viene scalato quando si usa il Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression Hypothesis\n",
    "$$ \\large h_{0}(x)=\\theta_{0} + \\theta_{1}x_{1} + \\theta_{1}x_{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggiungo la colonna di uno che pesano il parametro theta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "X_scaled = np.c_[np.ones((X_scaled.shape[0], 1)), X_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "L'unica differenza con la linear regression è che c'è un'altro example nella matrice X. La funzione di ipotesi e la regola di aggiornamento della discesa del gradiente batch rimangono invariate. Il modo più intelligente per affrontare questo problema è guardare all'implementazione vettoriale della discesa del gradiente, che è indipendente dal numero di features. Ricorda di aggiungere la colonna di 1 per il termine di intercetta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorized Implementation of the cost function**\n",
    "\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} (X\\theta - y)^T(X\\theta - y)$$\n",
    "\n",
    "**Vectorized Implementation of Gradient Descent** \n",
    "$$\\large \\theta = \\theta - \\frac{\\alpha}{m} ((X\\theta - y)^T X)^T = \\theta - \\frac{\\alpha}{m} (X^T(X\\theta - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentVectorized(X, y, theta=np.zeros((X.shape[1],1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters, dtype = 'float64')\n",
    "    \n",
    "    start = time.time()\n",
    "    for iter in np.arange(num_iters):\n",
    "        h = X.dot(theta)\n",
    "\n",
    "        # ! simoultaneusly update all the parameters \n",
    "        # vectorized implementation\n",
    "        theta = theta - alpha*(1.0/m)*(X.T.dot(h-y))\n",
    "        J_history[iter], _ = computeCostVectorized(X, y, theta)\n",
    "\n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    eta = end - start\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLoopNumpy(X, y, theta=np.zeros((X.shape[1],1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    start = time.time()\n",
    "    for iter in np.arange(num_iters):\n",
    "        \n",
    "        h = np.zeros((m,1))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                h[i] += theta[j] * X[i,j]\n",
    "\n",
    "        # ! simoultaneusly update all the parameters\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                gradient += (h[i]-y[i])*X[i,j]\n",
    "            theta[j] = theta[j] - alpha*(1/m)*gradient\n",
    "\n",
    "        J_history[iter], _ = computeCostLoopNumpy(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "    end = time.time()\n",
    "    eta = end - start\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addestrare il modello di Multivariate Linear Regression con e senza Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape[1])\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "theta[0] = 0\n",
    "theta[1] = 160\n",
    "theta[2] = 20000\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "\n",
    "theta_wfs, Cost_J_wfs, eta_wfs = gradientDescentVectorized(X, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = False)\n",
    "theta_fs, Cost_J_fs, eta_fs = gradientDescentVectorized(X_scaled, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = False)\n",
    "\n",
    "theta_wfs_np, Cost_J_wfs_np, eta_wfs_np = gradientDescentLoopNumpy(X, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = False)\n",
    "theta_fs_np, Cost_J_fs_np, eta_fs_np = gradientDescentLoopNumpy(X_scaled, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta values found by GD Vectorized without feature scaling : {}\".format(theta_wfs))\n",
    "print(\"theta values found by GD Vectorized with feature scaling : {}\".format(theta_fs))\n",
    "print(\"theta values found by GD Loop-based Numpy without feature scaling : {}\".format(theta_wfs_np))\n",
    "print(\"theta values found by GD Loop-based Numpy with feature scaling : {}\".format(theta_fs_np))\n",
    "\n",
    "# Optimal Value of theta : [340412.6679957  109447.78994015  -6578.35510528]\n",
    "# I valori nan sono causati da un'overflow poichè i valori non sono normalizzati o standardizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X_tmp = np.sort(np.c_[X_scaled[:,1], X_scaled[:,2], X_scaled.dot(theta_fs)], axis = 0)\n",
    "\n",
    "\n",
    "ax.scatter(X_scaled[:,1], X_scaled[:,2], y, c = 'r', label = 'training data')\n",
    "#ax.plot(X_scaled[:,1], X_scaled[:,2], X_scaled.dot(theta_fs), label = 'multivariate linear model')\n",
    "ax.plot(X_tmp[:,0], X_tmp[:,1], X_tmp[:,2], label = 'multivariate linear model')\n",
    "\n",
    "ax.set_xlabel('Size (in squared feet)')\n",
    "ax.set_ylabel('Number of bedrooms')\n",
    "ax.set_zlabel('Price')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title('Housing Prices in Portland (Oregon) - Multivariate Analysis', y = 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trovo $\\theta^{'}$ per gli input non scalati (closed form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}x_{1}^{(i)} + \\theta_{2}x_{2}^{(i)}$$\n",
    "\n",
    "$$\\large h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}\\frac{x_{1}^{(i)}-\\mu_1}{\\sigma_1} + \\theta_{2}\\frac{x_{2}^{(i)}-\\mu_2}{\\sigma_2}$$\n",
    "\n",
    "$$\\large \\sigma_1 \\sigma_2 h_{\\theta}(x^{(i)}) = \\sigma_1 \\sigma_2 \\theta_{0} + \\theta_{1}\\sigma_{2}(x_{1}^{(i)}-\\mu_1) + \\theta_{2}\\sigma_{1}(x_{2}^{(i)}-\\mu_2)$$\n",
    "\n",
    "$$\\large \\sigma_1 \\sigma_2 h_{\\theta}(x^{(i)}) = \\sigma_1 \\sigma_2 \\theta_{0} + \\theta_{1}\\sigma_{2}x_{1}^{(i)}-\\theta_{1}\\sigma_{2}\\mu_1 + \\theta_{2}\\sigma_{1}x_{2}^{(i)}-\\theta_{2}\\sigma_{1}\\mu_2$$\n",
    "\n",
    "$$\\large \\sigma_1 \\sigma_2 h_{\\theta}(x^{(i)}) = (\\sigma_1 \\sigma_2 \\theta_{0} -\\theta_{1}\\sigma_{2}\\mu_1 -\\theta_{2}\\sigma_{1}\\mu_2) + \\theta_{1}\\sigma_{2}x_{1}^{(i)} + \\theta_{2}\\sigma_{1}x_{2}^{(i)}$$\n",
    "\n",
    "$$\\large h_{\\theta}(x^{(i)}) = \\frac{(\\sigma_1 \\sigma_2 \\theta_{0} -\\theta_{1}\\sigma_{2}\\mu_1 -\\theta_{2}\\sigma_{1}\\mu_2)}{\\sigma_1\\sigma_2} + \\frac{\\theta_{1}x_{1}^{(i)}}{\\sigma_1} + \\frac{\\theta_{2}x_{2}^{(i)}}{\\sigma_{2}}$$\n",
    "\n",
    "$$\\large h_{\\theta^{'}}(x^{(i)}) = \\theta_{0}^{'} + \\theta_{1}^{'}x_{1}^{(i)} + \\theta_{2}^{'}x_{2}^{(i)}$$\n",
    "\n",
    "$$\\begin{equation}\\left\\{\\begin{aligned}&\\large \\theta_{0}^{'} = \\frac{(\\sigma_1 \\sigma_2 \\theta_{0} -\\theta_{1}\\sigma_{2}\\mu_1 -\\theta_{2}\\sigma_{1}\\mu_2)}{\\sigma_1\\sigma_2} \\\\&\\large \\theta_{1}^{'} = \\frac{\\theta_{1}}{\\sigma_1} \\\\&\\large \\theta_{2}^{'} = \\frac{\\theta_{2}}{\\sigma_2}\\end{aligned}\\right.\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General formulation\n",
    "$$\\large h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}\\frac{x_1^{(i)}-\\mu_1}{\\sigma_1} + \\theta_{2}\\frac{x_2^{(i)}-\\mu_2}{\\sigma_2} + \\dots + \\theta_{n}\\frac{x_n^{(i)}-\\mu_n}{\\sigma_n} $$\n",
    "\n",
    "$$ h_{\\theta}(x^{(i)}) = \\frac{[\\theta_{0}(\\sigma_1\\sigma_2\\sigma_3\\dots\\sigma_n) + \\theta_{1}(\\sigma_2\\sigma_3\\dots\\sigma_n)(x_1^{(i)}-\\mu_1) + \\theta_{2}(\\sigma_1\\sigma_3\\dots\\sigma_n)(x_2^{(i)}-\\mu_2) + \\dots + \\theta_{n}(\\sigma_1\\sigma_2\\dots\\sigma_{n-1})(x_n^{(i)}-\\mu_n)]}{\\sigma_1\\sigma_2\\sigma_3\\dots\\sigma_n} $$\n",
    "\n",
    "$$ h_{\\theta}(x^{(i)}) = \\frac{[\\theta_{0}(\\sigma_1\\sigma_2\\sigma_3\\dots\\sigma_n) - \\theta_1(\\sigma_2\\sigma_3\\dots\\sigma_n)\\mu_1 - \\theta_2(\\sigma_1\\sigma_3\\dots\\sigma_n)\\mu_2 + \\dots - \\theta_n(\\sigma_1\\sigma_2\\dots\\sigma_{n-1})\\mu_n + \\theta_1(\\sigma_2\\sigma_3\\dots\\sigma_n)x_1^{(i)} + \\theta_2(\\sigma_1\\sigma_3\\dots\\sigma_n)x_2^{(i)} + \\dots + \\theta_n(\\sigma_1\\sigma_2\\dots\\sigma_{n-1})x_n^{(i)}]}{\\sigma_1\\sigma_2\\sigma_3\\dots\\sigma_n} $$\n",
    "\n",
    "$$\\left\\{\\begin{aligned}&\\large h_{\\theta^\\prime}(x^{(i)}) = \\theta_{0}^\\prime + \\theta_{1}^\\prime x_{1}^{(i)} + \\theta_{2}^\\prime x_{2}^{(i)} + \\dots + \\theta_{n}^\\prime x_{n}^{(i)} \\\\ &\\large \\theta_0^\\prime = \\frac{\\theta_0\\prod_{i=1}^n\\sigma_i -\\sum_{i=1}^n{\\theta_i\\mu_i\\prod_{j\\neq i}^n\\sigma_j}}{\\prod_{i=1}^n\\sigma_i} = \\theta_0 - \\sum_{i=1}^n\\frac{\\theta_i}{\\sigma_i}\\mu_i = \\theta_0 - \\sum_{i=1}^n\\theta_i^\\prime\\mu_i \\\\ &\\large \\theta_i^\\prime = \\frac{\\theta_i}{\\sigma_i},\\ \\forall i = 1,\\dots,n\\end{aligned}\\right.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eseguo una predizione su un example mai visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output senza scaling\n",
    "x_tmp = np.array([1,1650,3])\n",
    "yhat_tmp = np.dot(x_tmp,theta_fs)\n",
    "print('yhat tmp : {}'.format(yhat_tmp))\n",
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# Remember to scale the new input\n",
    "#Testiamo il modello con nuovi valori\n",
    "x = np.array([1650, 3])\n",
    "#Tali nuovi valori devono essere scalati\n",
    "x = np.array([1, (x[0]-mu_X[0])/sigma_X[0], (x[1]-mu_X[1])/sigma_X[1]]).reshape(1,-1)\n",
    "\n",
    "#Eseguo il prodotto vettoriale tra i parametri theta del modello trovati ed i nuovi valori passati\n",
    "#x ha dimensione 1x3 e theta_fs di 3x1, quindi ottengo uno scalare di dimensione 1x1\n",
    "yhat_scaled = np.dot(x, theta_fs) \n",
    "print('Case with Feature Scaling : Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): {}'.format(yhat_scaled))\n",
    "\n",
    "#Poichè i theta sono scalati non posso applicare i nuovi dati della casa direttamente ma bisogna riportare i valori di theta scalati a quelli normali\n",
    "#per poi eseguire la predizione\n",
    "x1 = np.array([1,1650,3])\n",
    "thetap = np.copy(theta_fs)\n",
    "thetap[0] = (sigma_X[0]*sigma_X[1]*theta_fs[0] - theta_fs[1]*sigma_X[1]*mu_X[0] - theta_fs[2]*sigma_X[0]*mu_X[1])/(sigma_X[0]*sigma_X[1])\n",
    "thetap[1] = theta_fs[1]/sigma_X[0]\n",
    "thetap[2] = theta_fs[2]/sigma_X[1]\n",
    "yhat = np.dot(x1, thetap) \n",
    "print('Case without Feature Scaling : Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): {}'.format(yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta found by GD Vectorized with Feature Scaling : {}\".format(theta_fs))\n",
    "print(\"theta found by the closed form (for unscaled input) : {}\".format(thetap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confrontiamo i valori $\\theta^{'}$ con quelli trovati con Sklearn (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Scikit-learn Linear regression \n",
    "regr = LinearRegression()\n",
    "regr.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta found by Linear Regression Sklearn (OLS) (for unscaled input) : {}\".format([regr.intercept_, regr.coef_[1], regr.coef_[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esiste una soluzione in forma chiusa alla regressione lineare. È possibile applicando l'equazione normale che porta ai valori dei parametri migliori senza iterazioni.\n",
    "$$ \\large \\theta = (X^{T}X)^{-1}X^{T}y $$\n",
    "$$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m} (h_{\\theta}(x_{i})-y_{i})^{2} $$\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta-y)^{T}(X\\theta-y) $$\n",
    "$$ = ((X\\theta)^{T}-y^{T})(X\\theta-y) $$\n",
    "$$ = (X\\theta)^{T}(X\\theta)-(X\\theta)^{T}y-y^{T}X\\theta+y^{T}y $$\n",
    "$$ = \\theta^{T}X^{T}X\\theta-2(X\\theta)^{T}y+y^{T}y $$\n",
    "$$ = \\theta^{T}X^{T}X\\theta-2\\theta^{T}X^{T}y+y^{T}y $$\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta} = 2X^{T}X\\theta-2X^{T}y=0 $$\n",
    "$$ X^{T}X\\theta=X^{T}y $$\n",
    "$$ \\theta=(X^{T}X)^{-1}X^{T}y $$\n",
    "L'uso di questa formula non richiede alcun ridimensionamento delle features e si ottiene una soluzione esatta in un calcolo: non esiste un \"ciclo fino alla convergenza\" come nella discesa del gradiente. Ricorda che non c'è bisogno di ridimensionare le features, dobbiamo comunque aggiungere una colonna di 1 alla matrice X per avere un termine di intercetta theta0. NormalEquation() implementa la formula sopra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X è 47x3\n",
    "#y è 47x1\n",
    "#quindi theta è (3x47)(47x1)=3x1\n",
    "def normalEquations(X,y):\n",
    "    start = time.time()\n",
    "    #Calcolo la pseudo inversa anzichè l'inversa poichè il problema potrebbe essere malcondizionato (cioè ho feature ridondanti)\n",
    "    #oppure potrei avere più features rispetto i training examples; la pseudo inversa ovvia al problema stimando l'inversa. Essa è una matrice quadrata.\n",
    "    #Viene fatta per eliminare i punti di singolarità\n",
    "    pinv = np.linalg.pinv(X.T.dot(X))\n",
    "    theta_ne = pinv.dot(X.T).dot(y)\n",
    "    end = time.time()\n",
    "    eta_ne = end - start\n",
    "    return theta_ne, eta_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_ne, eta_ne = normalEquations(X,y)\n",
    "print(theta_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta found by GD Vectorized with Feature Scaling : {}\".format(theta_fs))\n",
    "print(\"theta found by the closed form (for unscaled input) : {}\".format(thetap))\n",
    "print(\"theta found by Linear Regression Sklearn (OLS) (for unscaled input) : {}\".format([regr.intercept_, regr.coef_[1], regr.coef_[2]]))\n",
    "print(\"theta found by Normal Equation : {}\".format(theta_ne))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((X.shape[1],1))\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "\n",
    "theta_vec_wfs, J_vec_wfs, eta_wfs_vec = gradientDescentVectorized(X, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = True)\n",
    "theta_vec_wfs, J_vec_fs, eta_fs_vec = gradientDescentVectorized(X_scaled, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = True)\n",
    "\n",
    "theta_lp_wfs_np, J_lp_wfs_np, eta_lp_wfs_np = gradientDescentLoopNumpy(X, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = True)\n",
    "theta_lp_fs_np, J_lp_fs_np, eta_lp_fs_np = gradientDescentLoopNumpy(X_scaled, y, theta = np.copy(theta), alpha = alpha, num_iters = num_iters, early = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The GD Vectorized without feature scaling stops at {} iterations\".format(len(J_vec_wfs)))\n",
    "print(\"The GD Vectorized with feature scaling stops at {} iterations\".format(len(J_vec_fs)))\n",
    "print(\"The GD Loop-based Numpy without feature scaling stops at {} iterations\".format(len(J_lp_wfs_np)))\n",
    "print(\"The GD Loop-based Numpy with feature scaling stops at {} iterations\".format(len(J_lp_fs_np)))\n",
    "print(\"\")\n",
    "print(\"Execution time GD Vectorized without features scaling until crash : {} [ms] {} [\\u03BC]\".format(round(eta_wfs_vec,6)*1000, round(eta_wfs_vec,6)*1000000))\n",
    "print(\"Execution time GD Vectorized with features scaling : {} [ms] {} [\\u03BC]\".format(round(eta_fs_vec,6)*1000, round(eta_fs_vec,6)*1000000))\n",
    "print(\"Execution time Normal Equation : {} [ms] {} [\\u03BC]\".format(round(eta_ne,6)*1000, round(eta_ne,6)*1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta found by GD Vectorized with scaled input & early stopping is : {}\".format(theta_vec_wfs))\n",
    "#I theta della Normal Equation sono uguali a quelli in forma chiusa\n",
    "print(\"theta found by Normal Equation is : {}\".format([theta_ne[0][0],theta_ne[1][0],theta_ne[2][0]]))\n",
    "\n",
    "#Verifichiamo che i theta trovati con il GD Vectorized, applicando le formule d'inversione mi ritornato quelli della Normal Equation\n",
    "theta0p = (sigma_X[0]*sigma_X[1]*theta_vec_wfs[0] - theta_vec_wfs[1]*sigma_X[1]*mu_X[0] - theta_vec_wfs[2]*sigma_X[0]*mu_X[1])/(sigma_X[0]*sigma_X[1])\n",
    "theta1p = theta_vec_wfs[1]/sigma_X[0]\n",
    "theta2p = theta_vec_wfs[2]/sigma_X[1]\n",
    "print(\"theta' found by formulas is : {}\".format([theta0p, theta1p, theta2p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1650,3])\n",
    "x = np.array([1, (x[0]-mu_X[0])/sigma_X[0], (x[1]-mu_X[1])/sigma_X[1]])\n",
    "\n",
    "x = x.reshape((1,x.shape[0]))\n",
    "price = x.dot(theta_ne)\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house(using normal equations): {}'.format(price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOMANDA: MEMORIZZARE LA NON INVERTIBILITA' SULL'INVERSA E LA PSEUDO INVERSA\n",
    "#### In matematica, e in particolare in algebra lineare, la matrice pseudo-inversa, o pseudo-inversa di Moore-Penrose, di una matrice data A si indica con A^{+} ed è la generalizzazione della matrice inversa al caso in cui A non sia quadrata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/Pseudo-inversa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
