{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:05:17.708474Z",
     "start_time": "2020-04-02T10:05:16.739778Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with one variable (Regression between #people and #profit in 10k units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:05:54.851053Z",
     "start_time": "2020-04-02T10:05:54.839578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 2)\n",
      "[[ 6.1101 17.592 ]\n",
      " [ 5.5277  9.1302]\n",
      " [ 8.5186 13.662 ]\n",
      " [ 7.0032 11.854 ]\n",
      " [ 5.8598  6.8233]\n",
      " [ 8.3829 11.886 ]\n",
      " [ 7.4764  4.3483]\n",
      " [ 8.5781 12.    ]\n",
      " [ 6.4862  6.5987]\n",
      " [ 5.0546  3.8166]]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('data/pop-profit.txt', delimiter=',')\n",
    "\n",
    "# print the shape of data\n",
    "print(data.shape)\n",
    "\n",
    "# print the first 10 elements, all columns\n",
    "print(data[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T10:24:57.346175Z",
     "start_time": "2020-03-11T10:24:57.341859Z"
    }
   },
   "source": [
    "**Non-vectorized hypothesis**\n",
    "$$\\large h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x, \\hspace{1pt} \\hspace{1cm} \\textit{where x is a single sample} $$ \n",
    "\n",
    "**Vectorized hypothesis**\n",
    "$$\\large h_{\\theta}(X) = X\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:06:14.283680Z",
     "start_time": "2020-04-02T10:06:14.280394Z"
    }
   },
   "outputs": [],
   "source": [
    "# add the column of ones for the intercept term in Linear Regression\n",
    "X = np.c_[np.ones(data.shape[0]), data[:,0]]\n",
    "y = np.c_[data[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:06:15.901129Z",
     "start_time": "2020-04-02T10:06:15.898074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 2)\n",
      "(97, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T09:41:45.599127Z",
     "start_time": "2020-03-25T09:41:45.595576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.     6.1101]\n",
      " [1.     5.5277]\n",
      " [1.     8.5186]\n",
      " [1.     7.0032]\n",
      " [1.     5.8598]]\n",
      "[[17.592 ]\n",
      " [ 9.1302]\n",
      " [13.662 ]\n",
      " [11.854 ]\n",
      " [ 6.8233]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:5, :])\n",
    "print(y[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Cost Function (theory recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Model (Non-vectorized):**\n",
    "$$\\large \\widehat{y}_{i} = h_{\\theta}(x_{i}) = x_{i}\\theta_1 + \\theta_0$$\n",
    "\n",
    "**Linear Regression Model (Vectorized):**\n",
    "$$\\large \\widehat{y} = h_{\\theta}(X) = X \\theta$$\n",
    "\n",
    "**Dimensionality Sanity Check:**\n",
    "$$\\large dim[\\widehat y_{i}] = dim[x_{i}\\theta_1 + \\theta_0] = 1 \\times n \\cdot n\\times 1 + 1\\times1 = 1 \\times 1$$\n",
    "\n",
    "$$\\large dim[\\widehat y] = dim[X \\theta] = m \\times n \\cdot n \\times 1 = m \\times 1$$\n",
    "**Non-Vectorized Implementation:**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "**Vectorized Implementation:**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} (X\\theta - y)^{T}(X\\theta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T11:08:36.723208Z",
     "start_time": "2020-03-19T11:08:36.718949Z"
    }
   },
   "source": [
    "**Vectorized Implementation:**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} (X\\theta - y)^{T}(X\\theta - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:06:45.539630Z",
     "start_time": "2020-04-02T10:06:45.535453Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeCostVectorized(X, y, theta=np.zeros((2, 1))):\n",
    "    m = y.size\n",
    "    J = 0\n",
    "\n",
    "    start = time.time()\n",
    "    h = X.dot(theta)\n",
    "    \n",
    "    # vectorized implementation\n",
    "    J = 1/(2*m)*((h-y).T.dot(h-y))\n",
    "    end = time.time()\n",
    "    eta = end-start\n",
    "    return(J, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Loop-based Implementation with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Hypothesis (Non-vectorized):**\n",
    "$$\\large \\widehat{y}_{i} = h_{\\theta}(x_{i}) = x_{i}\\theta_1 + \\theta_0$$\n",
    "\n",
    "**Non-Vectorized Implementation:**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:06:59.618532Z",
     "start_time": "2020-04-02T10:06:59.613089Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeCostLoopNumpy(X, y, theta=np.zeros((2, 1))):\n",
    "    m = y.size\n",
    "    J = 0\n",
    "    h = np.zeros((m,1))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # hypothesis evaluation loop-based implementation\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            #h[i] = h[i] + theta[j] * X[i,j]\n",
    "            h[i] += theta[j] * X[i,j]\n",
    "    \n",
    "    # semi-vectorized implementation (Numpy)\n",
    "    J = 1/(2*m)*np.sum(np.square(h-y))\n",
    "    end = time.time()\n",
    "    eta = end-start\n",
    "    return(J, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Custom Loop-based Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Hypothesis (Non-vectorized):**\n",
    "$$\\large \\widehat{y}_{i} = h_{\\theta}(x_{i}) = x_{i}\\theta_1 + \\theta_0$$\n",
    "\n",
    "**Non-Vectorized Implementation:**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:07:50.617758Z",
     "start_time": "2020-04-02T10:07:50.613274Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeCostLoopCustom(X, y, theta=np.zeros((2, 1))):\n",
    "    m = y.size\n",
    "    J = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # non-vectorized implementation (Custom)\n",
    "    for i in range(X.shape[0]):\n",
    "        h = 0\n",
    "        for j in range(X.shape[1]):\n",
    "            h += theta[j] * X[i,j]\n",
    "        J += (h - y[i])**2\n",
    "    \n",
    "    end = time.time()\n",
    "    eta = end-start\n",
    "    return(J/(2*m), eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between different Cost function implementations in terms of execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:08:37.353509Z",
     "start_time": "2020-04-02T10:08:20.033764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.07273387745567 32.072733877455676 32.072733877455654\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "eta_vec_list = []\n",
    "eta_ln_list = []\n",
    "eta_lc_list = []\n",
    "\n",
    "for i in range(count):\n",
    "    J_vec, eta_vec = computeCostVectorized(X, y)\n",
    "    eta_vec_list.append(eta_vec)\n",
    "    \n",
    "    J_ln, eta_ln = computeCostLoopNumpy(X, y)\n",
    "    eta_ln_list.append(eta_ln)\n",
    "    \n",
    "    J_lc, eta_lc = computeCostLoopCustom(X, y)\n",
    "    eta_lc_list.append(eta_lc)\n",
    "    \n",
    "print(J_vec[0][0], J_ln, J_lc[0])\n",
    "\n",
    "# check if the cost function implementations are correct\n",
    "# the assert returns an error if the argument is evaluated as false\n",
    "assert(round(J_vec[0][0], 3) == round(J_ln, 3) == round(J_lc[0], 3))\n",
    "\n",
    "# compute the average of eta values in each list\n",
    "mean_eta_vec = np.mean(eta_vec_list)\n",
    "mean_eta_ln = np.mean(eta_ln_list)\n",
    "mean_eta_lc = np.mean(eta_lc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:08:39.301944Z",
     "start_time": "2020-04-02T10:08:39.298467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function Vectorized value : 32.07273387745567\n",
      "Cost function Loop-based Numpy value : 32.072733877455676\n",
      "Cost function Loop-based Custom value : 32.072733877455654\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost function Vectorized value : {}\".format(J_vec[0][0]))\n",
    "print(\"Cost function Loop-based Numpy value : {}\".format(J_ln))\n",
    "print(\"Cost function Loop-based Custom value : {}\".format(J_lc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:08:54.409936Z",
     "start_time": "2020-04-02T10:08:54.405042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Cost Vectorized (Average time over 10000 iterations) 0.006 [ms] 5.831 [µs]\n",
      "Compute Cost Loop-based Numpy (Average time over 10000 iterations) 0.601 [ms] 600.816 [µs]\n",
      "Compute Cost Loop-based Custom (Average time over 10000 iterations) 0.748 [ms] 748.401 [µs]\n"
     ]
    }
   ],
   "source": [
    "print(\"Compute Cost Vectorized (Average time over {} iterations) {} [ms] {} [µs]\".format(count, round(mean_eta_vec*1000, 3), round(mean_eta_vec*1000000, 3)))\n",
    "print(\"Compute Cost Loop-based Numpy (Average time over {} iterations) {} [ms] {} [µs]\".format(count, round(mean_eta_ln*1000, 3), round(mean_eta_ln*1000000, 3)))\n",
    "print(\"Compute Cost Loop-based Custom (Average time over {} iterations) {} [ms] {} [µs]\".format(count, round(mean_eta_lc*1000, 3), round(mean_eta_lc*1000000, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Batch) Gradient Descent (theory recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-vectorized hypothesis**\n",
    "$$\\large h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x$$\n",
    "\n",
    "**Non-vectorized cost function**\n",
    "$$\\large J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "**Gradient of the cost function wrt the parameter $\\theta_{j}$**\n",
    "$$\\large \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j} \\rightarrow \\frac{1}{m} (X^T(X\\theta - y)$$\n",
    "\n",
    "**Standard Implementation of Gradient Descent (ved. slide 130)**\n",
    "$$\\large \\textit{Repeat until convergence} \\hspace{1cm} \\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j} \\hspace{1cm} \\textit{for} \\hspace{1cm} \\theta_{0}, \\theta_{1}, \\dots , \\theta_{n}$$\n",
    "\n",
    "**Vectorized Implementation of Gradient Descent** \n",
    "$$\\large \\theta = \\theta - \\frac{\\alpha}{m} ((X\\theta - y)^T X)^T = \\theta - \\frac{\\alpha}{m} (X^T(X\\theta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorized Implementation of Gradient Descent** \n",
    "$$\\large \\theta = \\theta - \\frac{\\alpha}{m} ((X\\theta - y)^T X)^T = \\theta - \\frac{\\alpha}{m} (X^T(X\\theta - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:10:08.915796Z",
     "start_time": "2020-04-02T10:10:08.910296Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    start = time.time()\n",
    "    for iter in np.arange(num_iters):\n",
    "        h = X.dot(theta)\n",
    "        \n",
    "        # ! simoultaneusly update all the parameters \n",
    "        # vectorized implementation\n",
    "        theta = theta - alpha*(1/m)*(X.T.dot(h-y))\n",
    "        J_history[iter], _ = computeCostVectorized(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "    end = time.time()\n",
    "    eta = end - start\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:10:10.826548Z",
     "start_time": "2020-04-02T10:10:10.605995Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_vec_es, J_history_vec_es, eta_vec_es = gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = True)\n",
    "theta_vec_nes, J_history_vec_nes, eta_vec_nes = gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = False)\n",
    "#mean_eta_vec_es = np.mean(eta_vec_es)\n",
    "#mean_eta_vec_nes = np.mean(eta_vec_nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:10:27.074229Z",
     "start_time": "2020-04-02T10:10:27.070135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Vectorized (Early Stopping) / Execution time 87.0 [ms]; 87000.0 [µs] over 8000 iterations\n",
      "Gradient Descent Vectorized (Without Early Stopping) / Execution time 93.0 [ms]; 93000.0 [µs] over 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Descent Vectorized (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_vec_es,3)*1000, round(eta_vec_es,3)*1000000, J_history_vec_es.shape[0]))\n",
    "print(\"Gradient Descent Vectorized (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_vec_nes,3)*1000, round(eta_vec_nes,3)*1000000, J_history_vec_nes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T15:46:21.680774Z",
     "start_time": "2020-03-18T15:46:21.678574Z"
    }
   },
   "source": [
    "## Gradient Descent Loop-based with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Model (Non-vectorized):**\n",
    "$$\\large \\widehat{y}_{i} = h_{\\theta}(x_{i}) = x_{i}\\theta_1 + \\theta_0$$\n",
    "\n",
    "$$\\large \\textit{Repeat until convergence} \\hspace{1cm} \\theta_{j} := \\theta_{j} - \\frac{\\alpha}{m}\\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j} \\hspace{1cm} \\textit{for} \\hspace{1cm} \\theta_{0}, \\theta_{1}, \\dots , \\theta_{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:10:59.151367Z",
     "start_time": "2020-04-02T10:10:59.143577Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentLoopNumpy(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    start = time.time()\n",
    "    for iter in np.arange(num_iters):\n",
    "        \n",
    "        h = np.zeros((m,1))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                h[i] += theta[j] * X[i,j]\n",
    "\n",
    "        # ! simoultaneusly update all the parameters\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                gradient += (h[i]-y[i])*X[i,j]\n",
    "            theta[j] = theta[j] - alpha*(1/m)*gradient\n",
    "\n",
    "        J_history[iter], _ = computeCostLoopNumpy(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "    end = time.time()\n",
    "    eta = end - start\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:11:41.696238Z",
     "start_time": "2020-04-02T10:11:01.139315Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_ln_es, J_history_ln_es, eta_ln_es = gradientDescentLoopNumpy(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = True)\n",
    "theta_ln_nes, J_history_ln_nes, eta_ln_nes = gradientDescentLoopNumpy(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = False)\n",
    "#mean_eta_ln_es = np.mean(eta_ln_es)\n",
    "#mean_eta_ln_nes = np.mean(eta_ln_nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:11:50.440204Z",
     "start_time": "2020-04-02T10:11:50.436515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Loop-based Numpy (Early Stopping) / Execution time 14149.0 [ms]; 14149000.0 [µs] over 7915 iterations\n",
      "Gradient Descent Loop-based Numpy (Without Early Stopping) / Execution time 17811.0 [ms]; 17811000.0 [µs] over 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Descent Loop-based Numpy (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_ln_es,3)*1000, round(eta_ln_es,3)*1000000, J_history_ln_es.shape[0]))\n",
    "print(\"Gradient Descent Loop-based Numpy (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_ln_nes,3)*1000, round(eta_ln_nes,3)*1000000, J_history_ln_nes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T15:46:50.151148Z",
     "start_time": "2020-03-18T15:46:50.148950Z"
    }
   },
   "source": [
    "## Gradient Descent Loop-based Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:12:28.108983Z",
     "start_time": "2020-04-02T10:12:28.101564Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentLoopCustom(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    start = time.time()\n",
    "    for iter in np.arange(num_iters):\n",
    "        \n",
    "        h = np.zeros((m,1))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                h[i] += theta[j] * X[i,j]\n",
    "\n",
    "        # ! simoultaneusly update all the parameters\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]): \n",
    "                gradient += (h[i]-y[i])*X[i,j]\n",
    "            theta[j] = theta[j] - alpha*(1/m)*gradient\n",
    "\n",
    "        J_history[iter], _ = computeCostLoopCustom(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "    end = time.time()\n",
    "    eta = end - start\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:13:12.859614Z",
     "start_time": "2020-04-02T10:12:29.845682Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_lc_es, J_history_lc_es, eta_lc_es = gradientDescentLoopCustom(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = True)\n",
    "theta_lc_nes, J_history_lc_nes, eta_lc_nes = gradientDescentLoopCustom(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000, early = False)\n",
    "#mean_eta_lc_es = np.mean(eta_lc_es)\n",
    "#mean_eta_lc_nes = np.mean(eta_lc_nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:13:12.952780Z",
     "start_time": "2020-04-02T10:13:12.949117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Loop-based Custom (Early Stopping) / Execution time 14522.0 [ms]; 14522000.0 [µs] over 7630 iterations\n",
      "Gradient Descent Loop-based Custom (Without Early Stopping) / Execution time 18780.0 [ms]; 18780000.0 [µs] over 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Descent Loop-based Custom (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_lc_es,3)*1000, round(eta_lc_es,3)*1000000, J_history_lc_es.shape[0]))\n",
    "print(\"Gradient Descent Loop-based Custom (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_lc_nes,3)*1000, round(eta_lc_nes,3)*1000000, J_history_lc_nes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved results for the GD implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:13:43.465711Z",
     "start_time": "2020-04-02T10:13:43.458045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Vectorized (Early Stopping) / Execution time 87.0 [ms]; 87000.0 [µs] over 8000 iterations\n",
      "Gradient Descent Vectorized (Without Early Stopping) / Execution time 93.0 [ms]; 93000.0 [µs] over 10000 iterations\n",
      "\n",
      "\n",
      "Gradient Descent Loop-based Numpy (Early Stopping) / Execution time 14149.0 [ms]; 14149000.0 [µs] over 7915 iterations\n",
      "Gradient Descent Loop-based Numpy (Without Early Stopping) / Execution time 17811.0 [ms]; 17811000.0 [µs] over 10000 iterations\n",
      "\n",
      "\n",
      "Gradient Descent Loop-based Custom (Early Stopping) / Execution time 14522.0 [ms]; 14522000.0 [µs] over 7630 iterations\n",
      "Gradient Descent Loop-based Custom (Without Early Stopping) / Execution time 18780.0 [ms]; 18780000.0 [µs] over 10000 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Descent Vectorized (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_vec_es,3)*1000, round(eta_vec_es,3)*1000000, J_history_vec_es.shape[0]))\n",
    "print(\"Gradient Descent Vectorized (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_vec_nes,3)*1000, round(eta_vec_nes,3)*1000000, J_history_vec_nes.shape[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Gradient Descent Loop-based Numpy (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_ln_es,3)*1000, round(eta_ln_es,3)*1000000, J_history_ln_es.shape[0]))\n",
    "print(\"Gradient Descent Loop-based Numpy (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_ln_nes,3)*1000, round(eta_ln_nes,3)*1000000, J_history_ln_nes.shape[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Gradient Descent Loop-based Custom (Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_lc_es,3)*1000, round(eta_lc_es,3)*1000000, J_history_lc_es.shape[0]))\n",
    "print(\"Gradient Descent Loop-based Custom (Without Early Stopping) / Execution time {} [ms]; {} [µs] over {} iterations\".format(round(eta_lc_nes,3)*1000, round(eta_lc_nes,3)*1000000, J_history_lc_nes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between different implementations and sanity check on $\\theta$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:14:54.960868Z",
     "start_time": "2020-04-02T10:14:08.030932Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1))\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "\n",
    "theta_vec, Cost_J_vec, eta_gd_vec = gradientDescentVectorized(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "theta_ln, Cost_J_ln, eta_gd_ln = gradientDescentLoopNumpy(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "theta_lc, Cost_J_lc, eta_gd_lc = gradientDescentLoopCustom(X, y, np.copy(theta), alpha, num_iters, early = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:14:57.752454Z",
     "start_time": "2020-04-02T10:14:57.746867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta found by GD Vectorized :  [-3.89578082  1.19303364]\n",
      "theta found by GD Loop-based Numpy :  [-3.89578082  1.19303364]\n",
      "theta found by GD Loop-based Custom :  [-3.89578082  1.19303364]\n"
     ]
    }
   ],
   "source": [
    "assert(round(theta_vec[0], 6) == round(theta_ln[0], 6) == round(theta_lc[0], 6))\n",
    "assert(round(theta_vec[1], 6) == round(theta_ln[1], 6) == round(theta_lc[1], 6))\n",
    "print('theta found by GD Vectorized : ', theta_vec)\n",
    "print('theta found by GD Loop-based Numpy : ', theta_ln)\n",
    "print('theta found by GD Loop-based Custom : ', theta_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **iterations** list contains different number of iterations to train the GD algorithm\n",
    "* Note that each algorithm is executed only ONE time for a fixed iteration number\n",
    "   * For example: when iteration is 1000, the GD algorithms are called just one time and interally they iterate for 1000 iterations\n",
    "   * In a more complex and precise case, the entire procedure should be repeated several times and the collected metrics should be averaged; it requires to add an inner-loop that loops for an arbitrary number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T10:15:23.684818Z",
     "start_time": "2020-04-02T10:15:23.681334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark iterations list:  [ 1000  2000  3000  4000  5000  6000  7000  8000  9000 10000]\n"
     ]
    }
   ],
   "source": [
    "start_iter = 1000\n",
    "end_iter = 11000\n",
    "iterations = np.arange(start_iter, end_iter, 1000)\n",
    "print(\"Benchmark iterations list: \", iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:25:47.815381Z",
     "start_time": "2020-04-02T10:41:19.505357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 10 (The GD algorithm runs for 1000 iterations)\n",
      "Inner Loop Iteration 1 of 10\n",
      "   > GD Vectorized is running...\n",
      "   > GD Vectorized completed\n",
      "\n",
      "   > GD Loop-based Numpy is running...\n",
      "   > GD Loop-based Numpy completed\n",
      "\n",
      "   > GD Loop-based Custom is running...\n",
      "   > GD Loop-based Custom completed\n",
      "\n",
      "Inner Loop Iteration 2 of 10\n",
      "   > GD Vectorized is running...\n",
      "   > GD Vectorized completed\n",
      "\n",
      "   > GD Loop-based Numpy is running...\n",
      "   > GD Loop-based Numpy completed\n",
      "\n",
      "   > GD Loop-based Custom is running...\n",
      "   > GD Loop-based Custom completed\n",
      "\n",
      "Inner Loop Iteration 3 of 10\n",
      "   > GD Vectorized is running...\n",
      "   > GD Vectorized completed\n",
      "\n",
      "   > GD Loop-based Numpy is running...\n",
      "   > GD Loop-based Numpy completed\n",
      "\n",
      "   > GD Loop-based Custom is running...\n",
      "   > GD Loop-based Custom completed\n",
      "\n",
      "Inner Loop Iteration 4 of 10\n",
      "   > GD Vectorized is running...\n",
      "   > GD Vectorized completed\n",
      "\n",
      "   > GD Loop-based Numpy is running...\n",
      "   > GD Loop-based Numpy completed\n",
      "\n",
      "   > GD Loop-based Custom is running...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-01bbabd234da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   > GD Loop-based Numpy completed\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   > GD Loop-based Custom is running...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtheta_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCost_J_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_gd_lc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradientDescentLoopCustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   > GD Loop-based Custom completed\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0meta_gd_vec_list_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta_gd_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2dd9eb0f551d>\u001b[0m in \u001b[0;36mgradientDescentLoopCustom\u001b[0;34m(X, y, theta, alpha, num_iters, early)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mgradient\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inner_loop_counter = 10\n",
    "\n",
    "theta = np.zeros((2,1))\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "eta_gd_vec_list = []\n",
    "eta_gd_ln_list = []\n",
    "eta_gd_lc_list = []\n",
    "\n",
    "for idx, iteration in enumerate(iterations):\n",
    "    eta_gd_vec_list_inner = []\n",
    "    eta_gd_ln_list_inner = []\n",
    "    eta_gd_lc_list_inner = []\n",
    "    print(\"Iteration {} of {} (The GD algorithm runs for {} iterations)\".format(idx+1, iterations.shape[0], iterations[idx]))\n",
    "    for j in range(inner_loop_counter):\n",
    "        print(\"Inner Loop Iteration {} of {}\".format(j+1, inner_loop_counter))\n",
    "        print(\"   > GD Vectorized is running...\")\n",
    "        theta_vec, Cost_J_vec, eta_gd_vec = gradientDescentVectorized(X, y, np.copy(theta), alpha, iteration, early = False)\n",
    "        print(\"   > GD Vectorized completed\\n\")\n",
    "        print(\"   > GD Loop-based Numpy is running...\")\n",
    "        theta_ln, Cost_J_ln, eta_gd_ln = gradientDescentLoopNumpy(X, y, np.copy(theta), alpha, iteration, early = False)\n",
    "        print(\"   > GD Loop-based Numpy completed\\n\")\n",
    "        print(\"   > GD Loop-based Custom is running...\")\n",
    "        theta_lc, Cost_J_lc, eta_gd_lc = gradientDescentLoopCustom(X, y, np.copy(theta), alpha, iteration, early = False)\n",
    "        print(\"   > GD Loop-based Custom completed\\n\")\n",
    "        eta_gd_vec_list_inner.append(eta_gd_vec)\n",
    "        eta_gd_ln_list_inner.append(eta_gd_ln)\n",
    "        eta_gd_lc_list_inner.append(eta_gd_lc)\n",
    "        \n",
    "    \n",
    "    eta_gd_vec_list.append(np.mean(eta_gd_vec_list_inner))\n",
    "    eta_gd_ln_list.append(np.mean(eta_gd_ln_list_inner))\n",
    "    eta_gd_lc_list.append(np.mean(eta_gd_lc_list_inner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Express the execution time in milliseconds and microseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:26:57.157742Z",
     "start_time": "2020-04-02T11:26:57.152899Z"
    }
   },
   "outputs": [],
   "source": [
    "# execution time in milliseconds\n",
    "eta_gd_vec_list_ms = [i*1000 for i in eta_gd_vec_list]\n",
    "eta_gd_ln_list_ms = [i*1000 for i in eta_gd_ln_list]\n",
    "eta_gd_lc_list_ms = [i*1000 for i in eta_gd_lc_list]\n",
    "\n",
    "# execution time in microseconds\n",
    "eta_gd_vec_list_us = [i*1000000 for i in eta_gd_vec_list]\n",
    "eta_gd_ln_list_us = [i*1000000 for i in eta_gd_ln_list]\n",
    "eta_gd_lc_list_us = [i*1000000 for i in eta_gd_lc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Benchmark results (in milliseconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:27:29.600472Z",
     "start_time": "2020-04-02T11:27:29.104708Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_scale = 'log'\n",
    "\n",
    "if (plot_scale == 'linear'):\n",
    "    # plot in linear scale\n",
    "    scale = 'linear'\n",
    "elif (plot_scale == 'log'):\n",
    "    # plot in semi-log scale (log scale on y-axis)\n",
    "    scale = 'log'\n",
    "\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "plt.scatter(iterations, eta_gd_vec_list_ms, marker = 'x', c = 'r', s = 50, label = 'Gradient Descent Vectorized')\n",
    "plt.plot(iterations, eta_gd_vec_list_ms, c = 'r')\n",
    "plt.scatter(iterations, eta_gd_ln_list_ms, marker = 'x', c = 'g', s = 50, label = 'Gradient Descent Loop-based Numpy')\n",
    "plt.plot(iterations, eta_gd_ln_list_ms, c = 'g')\n",
    "plt.scatter(iterations, eta_gd_lc_list_ms, marker = 'x', c = 'orange', s = 50, label = 'Gradient Descent Loop-based Custom')\n",
    "plt.plot(iterations, eta_gd_lc_list_ms, c = 'orange')\n",
    "plt.yscale(scale)\n",
    "plt.xlabel('# Iterations', size = 15)\n",
    "plt.ylabel('Average Computation time [ms]', size = 15)\n",
    "plt.title('Linear Regression - Benchmarking', size = 15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the early stopping iteration for the Gradient Descent implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:27:51.830436Z",
     "start_time": "2020-04-02T11:27:31.667189Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1))\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "\n",
    "theta_vec_sp, Cost_J_vec_sp, eta_gd_vec_sp = gradientDescentVectorized(X, y, np.copy(theta), alpha, num_iters, early = True)\n",
    "theta_ln_sp, Cost_J_ln_sp, eta_gd_ln_sp = gradientDescentLoopNumpy(X, y, np.copy(theta), alpha, num_iters, early = True)\n",
    "theta_lc_sp, Cost_J_lc_sp, eta_gd_lc_sp = gradientDescentLoopCustom(X, y, np.copy(theta), alpha, num_iters, early = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:27:51.832570Z",
     "start_time": "2020-04-02T11:27:43.717Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Gradient Descent Vectorized with Early stopping stops at {} iterations\".format(len(Cost_J_vec_sp)))\n",
    "print(\"Gradient Descent Loop-based Numpy with Early stopping stops at {} iterations\".format(len(Cost_J_ln_sp)))\n",
    "print(\"Gradient Descent Loop-based Custom with Early stopping stops at {} iterations\".format(len(Cost_J_lc_sp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifica aggiuntiva early stopping con ulteriori grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_vec_es0, J_history_vec_es0, eta_vec_es0 = gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000 ,early = True)\n",
    "theta_vec_es1, J_history_vec_es1, eta_vec_es1 = gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=9000 ,early = False)\n",
    "theta_vec_es2, J_history_vec_es2, eta_vec_es2 = gradientDescentVectorized(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=10000 ,early = False)\n",
    "\n",
    "print(\"Gradient Descent Vectorized (Early Stopping) executed in {} [ms] with {} iterations\".format(round(eta_vec_es0,3)*1000, J_history_vec_es0.shape[0]))\n",
    "print(\"Gradient Descent Vectorized (Without Early Stopping 9000 iterations) executed in {} [ms] with {} iterations\".format(round(eta_vec_es1,3)*1000, J_history_vec_es1.shape[0]))\n",
    "print(\"Gradient Descent Vectorized (Without Early Stopping 10000 iterations) executed in {} [ms] with {} iterations\".format(round(eta_vec_es2,3)*1000, J_history_vec_es2.shape[0]))\n",
    "\n",
    "#Verifico plottando che nonostante il diverso numero di iterazioni il grafico dell'Early Stopping e quelli senza combacino \n",
    "def plotCost(j_history_vec2, label2, j_history_vec1, label1, j_history_vec0, label0):\n",
    "    fig=plt.figure(figsize=(20,5))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost J')\n",
    "    plt.plot(j_history_vec2, label=label2)\n",
    "    plt.plot(j_history_vec1, label=label1)\n",
    "    plt.plot(j_history_vec0, label=label0)\n",
    "    plt.legend(loc=1)\n",
    "    #plt.plot(J_history_vec_es1, label=\"Cost Loop-based (Numpy)\")\n",
    "    #plt.plot(J_history_vec_es2, label=\"Cost Loop-based (Custom)\")\n",
    "\n",
    "plotCost(J_history_vec_es2, 'Cost Vectorized without Early Stopping with 10000 iterations',J_history_vec_es1, 'Cost Vectorized without Early Stopping with 9000 iterations', J_history_vec_es0, 'Cost Vectorized with Early Stopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot funzione di costo con grafici a barre\n",
    "def plotBarChart(j_history_vec2, label2, j_history_vec1, label1, j_history_vec0, label0):\n",
    "    fig=plt.figure(figsize=(20,6))\n",
    "    plt.ylim(ymin=4.45,ymax=6)\n",
    "    plt.bar(np.arange(len(j_history_vec0)), j_history_vec0, color='g', label=label0)\n",
    "    plt.bar(np.arange(len(j_history_vec1)), j_history_vec1, color='b', label=label1)\n",
    "    plt.bar(np.arange(len(j_history_vec2)), j_history_vec2, color='orange', label=label2)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost J')\n",
    "\n",
    "plotBarChart(J_history_vec_es0, \"Last 5 cost values with Early Stop are: %.15f , %.15f , %.15f , %.15f, %.15f\" %(J_history_vec_es0[-5], J_history_vec_es0[-4], J_history_vec_es0[-3], J_history_vec_es0[-2], J_history_vec_es0[-1]),\n",
    "            J_history_vec_es1, \"Last 5 cost values without Early Stopping with 9000 iterations are: %.15f , %.15f , %.15f , %.15f, %.15f\" %(J_history_vec_es1[-5], J_history_vec_es1[-4], J_history_vec_es1[-3], J_history_vec_es1[-2], J_history_vec_es1[-1]),\n",
    "            J_history_vec_es2, \"Last 5 cost values without Early Stopping with 10000 iterations are: %.15f , %.15f , %.15f , %.15f, %.15f\" %(J_history_vec_es2[-5], J_history_vec_es2[-4], J_history_vec_es2[-3], J_history_vec_es2[-2], J_history_vec_es2[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking the inner loop iteration time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Move the **start** and **end** time statements inside the iteration loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T10:49:59.321489Z",
     "start_time": "2020-03-25T10:49:59.315658Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentVectorizedIter(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    eta = []\n",
    "    \n",
    "    for iter in np.arange(num_iters):\n",
    "        start = time.time()\n",
    "        h = X.dot(theta)\n",
    "        \n",
    "        # ! simoultaneusly update all the parameters \n",
    "        # vectorized implementation\n",
    "        theta = theta - alpha*(1/m)*(X.T.dot(h-y))\n",
    "        J_history[iter], _ = computeCostVectorized(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "        end = time.time()\n",
    "        eta.append(end - start)\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T10:50:00.271842Z",
     "start_time": "2020-03-25T10:50:00.263519Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentLoopNumpyIter(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    eta = []\n",
    "    \n",
    "    for iter in np.arange(num_iters):\n",
    "        start = time.time()\n",
    "        \n",
    "        h = np.zeros((m,1))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                h[i] += theta[j] * X[i,j]\n",
    "\n",
    "        # ! simoultaneusly update all the parameters\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                gradient += (h[i]-y[i])*X[i,j]\n",
    "            theta[j] = theta[j] - alpha*(1/m)*gradient\n",
    "\n",
    "        J_history[iter], _ = computeCostLoopNumpy(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "        end = time.time()\n",
    "        eta.append(end - start)\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T10:50:05.547739Z",
     "start_time": "2020-03-25T10:50:05.540050Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradientDescentLoopCustomIter(X, y, theta=np.zeros((2,1)), alpha=0.01, num_iters=1500, early = False):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    eta = []\n",
    "    \n",
    "    for iter in np.arange(num_iters):\n",
    "        start = time.time()\n",
    "        \n",
    "        h = np.zeros((m,1))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                h[i] += theta[j] * X[i,j]\n",
    "\n",
    "        # ! simoultaneusly update all the parameters\n",
    "        for j in range(X.shape[1]):\n",
    "            gradient = 0\n",
    "            for i in range(X.shape[0]): \n",
    "                gradient += (h[i]-y[i])*X[i,j]\n",
    "            theta[j] = theta[j] - alpha*(1/m)*gradient\n",
    "\n",
    "        J_history[iter], _ = computeCostLoopCustom(X, y, theta)\n",
    "        \n",
    "        # early stopping\n",
    "        if (early == True) & (J_history[iter] == J_history[iter-1]):\n",
    "            break\n",
    "            \n",
    "        end = time.time()\n",
    "        eta.append(end - start)\n",
    "    return(theta.ravel(), J_history[J_history != 0], eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T10:51:20.017308Z",
     "start_time": "2020-03-25T10:50:30.768251Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1))\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "\n",
    "_, _, eta_vec_iter = gradientDescentVectorizedIter(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "_, _, eta_ln_iter = gradientDescentLoopNumpyIter(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "_, _, eta_lc_iter = gradientDescentLoopCustomIter(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "\n",
    "mean_eta_vec_iter = np.mean(eta_vec_iter)\n",
    "mean_eta_ln_iter = np.mean(eta_ln_iter)\n",
    "mean_eta_lc_iter = np.mean(eta_lc_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T10:51:22.750377Z",
     "start_time": "2020-03-25T10:51:22.745704Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Gradient Descent Vectorized : {} [ms]/iteration; {} [µs]/iteration (Average over {} iterations)\".format(round(mean_eta_vec_iter, 6)*1000, round(mean_eta_vec_iter, 6)*1000000, num_iters))\n",
    "print(\"Gradient Descent Loop-based Numpy : {} [ms]/iteration; {} [µs]/iteration (Average over {} iterations)\".format(round(mean_eta_ln_iter, 6)*1000, round(mean_eta_ln_iter, 6)*1000000, num_iters))\n",
    "print(\"Gradient Descent Loop-based Custom : {} [ms]/iteration; {} [µs]/iteration (Average over {} iterations)\".format(round(mean_eta_lc_iter, 6)*1000, round(mean_eta_lc_iter, 6)*1000000, num_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking n1: Gradient Descent Vectorized Vs Normal Equation (Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation (Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\theta = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:28:09.782755Z",
     "start_time": "2020-04-02T11:28:09.778894Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalEquations(X, y):\n",
    "    start = time.time()\n",
    "    #theta = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(X), X)), np.transpose(X)), y)\n",
    "    pinv = np.linalg.pinv(X.T.dot(X))\n",
    "    theta_ne = pinv.dot(X.T).dot(y)\n",
    "    end = time.time()\n",
    "    eta_ne = end-start\n",
    "    return theta_ne.ravel(), eta_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:30:06.508074Z",
     "start_time": "2020-04-02T11:29:54.127439Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1))\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 10000\n",
    "num_iters_gd_vs_ne = 100\n",
    "\n",
    "eta_gd_bench_list_1 = []\n",
    "eta_ne_bench_list_1 = []\n",
    "\n",
    "for i in range(num_iters_gd_vs_ne):\n",
    "    print(\"Iteration {} of {}\".format(i+1, num_iters_gd_vs_ne))\n",
    "    _, _, eta_gd_vec_1 = gradientDescentVectorized(X, y, np.copy(theta), alpha, num_iters, early = False)\n",
    "    _, eta_ne_1 = normalEquations(X, y)\n",
    "    eta_gd_bench_list_1.append(eta_gd_vec_1)\n",
    "    eta_ne_bench_list_1.append(eta_ne_1)\n",
    "    \n",
    "mean_eta_gd_bench_1 = np.mean(eta_gd_bench_list_1)\n",
    "mean_eta_ne_bench_1 = np.mean(eta_ne_bench_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:31:06.723762Z",
     "start_time": "2020-04-02T11:31:06.720024Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average execution time Gradient Descent Vectorized : {} [ms] {} [µs]\".format(round(mean_eta_gd_bench_1, 6)*1000, round(mean_eta_gd_bench_1, 6)*1000000))\n",
    "print(\"Average execution time Normal Equation : {} [ms] {} [µs]\".format(round(mean_eta_ne_bench_1, 6)*1000, round(mean_eta_ne_bench_1, 6)*1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking n2: Normal Equation (Custom) Vs Linear Regression (Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:31:19.096565Z",
     "start_time": "2020-04-02T11:31:19.093291Z"
    }
   },
   "outputs": [],
   "source": [
    "def LR(X, y):\n",
    "    start = time.time()\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X,y)\n",
    "    end = time.time()\n",
    "    eta_lr = end - start\n",
    "    return regr, eta_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:31:37.951186Z",
     "start_time": "2020-04-02T11:31:37.867351Z"
    }
   },
   "outputs": [],
   "source": [
    "num_iters_ne_vs_lr = 100\n",
    "\n",
    "eta_ne_bench_list_2 = []\n",
    "eta_lr_bench_list_2 = []\n",
    "\n",
    "for i in range(num_iters_ne_vs_lr):\n",
    "    print(\"Iteration {} of {}\".format(i+1, num_iters_ne_vs_lr))\n",
    "    _, eta_ne_2 = normalEquations(X, y)\n",
    "    _, eta_lr_2 = LR(X, y)\n",
    "    \n",
    "    eta_ne_bench_list_2.append(eta_ne_2)\n",
    "    eta_lr_bench_list_2.append(eta_lr_2)\n",
    "    \n",
    "mean_eta_ne_bench_2 = np.mean(eta_ne_bench_list_2)\n",
    "mean_eta_lr_bench_2 = np.mean(eta_lr_bench_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:32:19.249744Z",
     "start_time": "2020-04-02T11:32:19.245684Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average execution time Normal Equation : {} [ms] {} [µs]\".format(round(mean_eta_ne_bench_2, 6)*1000, round(mean_eta_ne_bench_2, 6)*1000000))\n",
    "print(\"Average execution time Linear Regression (Sklearn) : {} [ms] {} [µs]\".format(round(mean_eta_lr_bench_2, 6)*1000, round(mean_eta_lr_bench_2, 6)*1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
